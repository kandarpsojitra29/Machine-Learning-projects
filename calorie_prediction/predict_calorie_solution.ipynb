{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f3005-5ea8-4b74-879e-ccab2652ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff860a-9e5a-4a76-b6dc-f034326755e5",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d5614-d097-44cb-b8f4-d3e837717aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = pd.read_csv('label_train.csv')\n",
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e18588-8e2e-44ff-87bf-818bae6fb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a0933-9952-4555-89d7-4a75995b5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing the data\n",
    "y_train = label_train.iloc[:,2:].values\n",
    "name = label_train.columns[2:].values\n",
    "\n",
    "fig , ax = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=4,\n",
    "    figsize=(15, 10))\n",
    "\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(y_train.shape[1]):\n",
    "    ax[i].hist(y_train[:,i],bins=20,color='b',edgecolor='k')\n",
    "    ax[i].set_title(name[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a44bf-7e38-47ce-94b7-72d7f4fd2cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data_preprocessing(df):\n",
    "    # List of columns to exclude from normalization\n",
    "    exclude_columns = ['Subject ID', 'Day']\n",
    "    columns_to_scale = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    df[columns_to_scale] = df[columns_to_scale].div(df[columns_to_scale].mean()) #normalize by mean\n",
    "    return df\n",
    "preprocessed_label = label_data_preprocessing(label_train)\n",
    "preprocessed_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b1618-13be-4e9f-a019-4bc389be0763",
   "metadata": {},
   "source": [
    "Lunch calories which we will select as our label later when we load our data into Dataloader are normalized by mean value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0481b0-3482-4912-9f32-e1cc99d3cee2",
   "metadata": {},
   "source": [
    "## Viome Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e88e3-3de0-4fc1-9689-e435edf66396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_viome_train = pd.read_csv('demo_viome_train.csv')\n",
    "demo_viome_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4632853-f4dd-450f-8408-69cc3f2735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_viome_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d2278-268d-43fc-8e97-aab2fdaaf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_empty_values = demo_viome_train[demo_viome_train['Viome'] == '[]']\n",
    "print(rows_with_empty_values['Viome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fb3d9-5d1a-4283-a1c3-9637c8295aa8",
   "metadata": {},
   "source": [
    "No empty value in Viome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442aca6e-a5c8-4547-9103-8639557a9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting features as x_train (numpy array) and visualizing the data\n",
    "x_train = demo_viome_train.iloc[:,2:-1].values\n",
    "name = demo_viome_train.columns[2:-1].values\n",
    "\n",
    "fig , ax = plt.subplots(\n",
    "    nrows=5,\n",
    "    ncols=4,\n",
    "    figsize=(15, 10))\n",
    "\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(x_train.shape[1]):\n",
    "    ax[i].hist(x_train[:,i],bins=20,color='b',edgecolor='k')\n",
    "    ax[i].set_title(name[i])\n",
    "\n",
    "fig.delaxes(ax[-3])\n",
    "fig.delaxes(ax[-2])\n",
    "fig.delaxes(ax[-1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7613a95-25cc-4a2b-96be-8980a9589db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Viome Data \n",
    "x_train = demo_viome_train.iloc[:,-1].values\n",
    "name = demo_viome_train.columns[-1]\n",
    "\n",
    "# Splitting the strings and converting to floats\n",
    "extracted_values = np.array([list(map(float, row.split(','))) for row in x_train])\n",
    "\n",
    "fig , ax = plt.subplots(\n",
    "    nrows=6,\n",
    "    ncols=6,\n",
    "    figsize=(15, 10))\n",
    "\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(len(extracted_values)):\n",
    "    ax[i].hist(extracted_values[i],bins=20,color='b',edgecolor='k')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751cedcd-9cba-4a4b-b64c-dc17bc5e1ed9",
   "metadata": {},
   "source": [
    "### Data preprocessing of demographic data and Viome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16fbee3-e3aa-4252-b51a-879298b0ff86",
   "metadata": {},
   "source": [
    "- For categoricala features, we applied one-hot encoding\n",
    "- For numerical features, we applied Min-Max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1a187-47dd-4607-aac6-8b2225ad6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def demo_viome_data_preprocessing(df):\n",
    "    # List of columns to exclude from scaling\n",
    "    exclude_columns = ['Subject ID', 'Gender', 'Race', 'Diabetes Status', 'Viome']\n",
    "    columns_to_scale = [col for col in df.columns if col not in exclude_columns]\n",
    "    # MinMaxScaler to columns_to_scale\n",
    "    scaler = MinMaxScaler()\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    \n",
    "    # One-Hot Encoding for 'Race' and 'Diabetes Status' columns\n",
    "    df = pd.get_dummies(df, columns=['Race', 'Diabetes Status'], drop_first=True)\n",
    "\n",
    "    df['Viome'] = df['Viome'].apply(lambda x: np.array([float(num) for num in x.split(',')]))\n",
    "\n",
    "    # Stack vectors to create a 2D array for scaling\n",
    "    viome_array = np.stack(df['Viome'].values)\n",
    "    \n",
    "    # Apply MinMaxScaler\n",
    "    viome_scaled = scaler.fit_transform(viome_array)\n",
    "    # Replace original column with the scaled values\n",
    "    df['Viome'] = list(viome_scaled)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "demo_viome_train_processed = demo_viome_data_preprocessing(demo_viome_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e47ef-84e3-4789-a84e-aeae002317d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_viome_train_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a32e2-b496-4717-87c0-91535b5d9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and names\n",
    "x_train = demo_viome_train_processed.drop(columns=['Viome']).values\n",
    "x_train = x_train.astype(float)\n",
    "name = demo_viome_train_processed.columns.difference(['Viome'], sort=False).tolist()\n",
    "\n",
    "# Determine number of features\n",
    "num_features = x_train.shape[1]\n",
    "rows, cols = 5, 5\n",
    "\n",
    "fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(15, 10))\n",
    "ax = ax.ravel()\n",
    "\n",
    "# Loop through each feature and plot histogram\n",
    "for i in range(num_features):\n",
    "    ax[i].hist(x_train[:, i], bins=20, color='b', edgecolor='k')\n",
    "    ax[i].set_title(name[i])\n",
    "\n",
    "for i in range(num_features, len(ax)):\n",
    "    fig.delaxes(ax[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5d02a-6f5f-41c7-bc36-886166a7d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalized Viome data\n",
    "x_train = demo_viome_train_processed['Viome'].values\n",
    "\n",
    "fig , ax = plt.subplots(\n",
    "    nrows=6,\n",
    "    ncols=6,\n",
    "    figsize=(15, 10))\n",
    "\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    ax[i].hist(x_train[i],bins=20,color='b',edgecolor='k')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330af7e9-c07d-4796-b819-0507b5dd2e97",
   "metadata": {},
   "source": [
    "## CGM Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072d5a4-0fb9-4b14-bc0e-1cef221c4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_train = pd.read_csv('cgm_train.csv')\n",
    "cgm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a94c3-2f4f-40c2-80b6-0bde1ac7cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858039d3-e63d-4a88-81e0-f6f441d7303a",
   "metadata": {},
   "source": [
    "We will preprocess CGM data after merging all dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626dac2b-c849-41b8-b5fc-4ed5191c7320",
   "metadata": {},
   "source": [
    "## Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456863d5-c4e1-4467-8c34-741b68073580",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = pd.read_csv('img_train.csv')\n",
    "img_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577520f4-143d-415f-a4d9-936fa6e9bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fab3de-658a-49e0-86a4-bdcf524f9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing the fiber data\n",
    "x_train = img_train.iloc[:,2:4].values\n",
    "name = img_train.columns[2:4].values\n",
    "\n",
    "fig , ax = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=2,\n",
    "    figsize=(8, 4))\n",
    "\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(x_train.shape[1]):\n",
    "    ax[i].hist(x_train[:,i],bins=20,color='b',edgecolor='k')\n",
    "    ax[i].set_title(name[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268c66b-cdd5-4e1c-ac78-7d37888d17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in 'Breakfast Fiber' with the median\n",
    "img_train['Breakfast Fiber'] = img_train['Breakfast Fiber'].fillna(img_train['Breakfast Fiber'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961a377-2cdd-4697-8f26-7a18243bff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd24e8-a4bc-4d6e-b31c-80e91b0186c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "rgb_image_str = img_train[\"Image Before Breakfast\"][0]\n",
    "rgb_image = ast.literal_eval(rgb_image_str)\n",
    "rgb_array = np.array(rgb_image)\n",
    "# Plotting the image\n",
    "plt.imshow(rgb_array)\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130fade-6867-45a8-b4b5-e8927a863c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image_str = img_train[\"Image Before Lunch\"][0]\n",
    "rgb_image = ast.literal_eval(rgb_image_str)\n",
    "rgb_array = np.array(rgb_image)\n",
    "# Plotting the image\n",
    "plt.imshow(rgb_array)\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(img_train[\"Image Before Lunch\"])):\n",
    "    rgb_image_str = img_train[\"Image Before Lunch\"][i]\n",
    "#     print(np.shape(np.array(ast.literal_eval(rgb_image_str))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d4cb1",
   "metadata": {},
   "source": [
    "## Merge the three dataframes and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddc8c2-3a23-4bcf-9fad-bf0106b38500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge the CGM_data and Image_data based on 'Subject ID' and 'Day' columns\n",
    "df1 = cgm_train\n",
    "df2 = img_train\n",
    "\n",
    "merged_df_train = pd.merge(cgm_train, img_train, on=['Subject ID', 'Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee215f51-7746-4e44-a0b2-d919a868673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_train_wdemo_wviome = merged_df_train.merge(\n",
    "    demo_viome_train_processed,\n",
    "    on=['Subject ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25dcc16-42f4-4e89-a3b7-24d9f48b7850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df_wlabels = pd.merge(merged_df_train_wdemo_wviome, preprocessed_label, on=['Subject ID', 'Day'])\n",
    "merged_df_wlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b16462",
   "metadata": {},
   "source": [
    "## Check for missing data and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9bd1b7-d30b-47b0-a87e-6d4d023abbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_empty_values = merged_df_wlabels[merged_df_wlabels['Image Before Lunch'] == '[]']\n",
    "print(rows_with_empty_values['Image Before Lunch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd78dee-d817-4dc8-bd6c-12bcbb0f1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_wlabels = merged_df_wlabels.drop(rows_with_empty_values.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a54a4b-d593-4411-99e7-3f19b667ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_empty_values = merged_df_wlabels[merged_df_wlabels['Image Before Breakfast'] == '[]']\n",
    "print(rows_with_empty_values['Image Before Breakfast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34db39-b641-46ff-bc5f-c105491cc2b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df_wlabels = merged_df_wlabels.drop(rows_with_empty_values.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67303e8b-cc4f-48de-8d43-bd9a1e51f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_empty_values = merged_df_wlabels[merged_df_wlabels['CGM Data'] == '[]']\n",
    "print(rows_with_empty_values['CGM Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda69388-21af-49ee-b69c-f38e94e5330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_wlabels = merged_df_wlabels.drop(rows_with_empty_values.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb8ed0-e350-4e5f-ac96-ebf0ed245fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df_wlabels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ee536",
   "metadata": {},
   "source": [
    "## Interpolate CGM data to per minute and Normalize by Baseline Fasting Glucose "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bcd3e4-5b6d-4cd8-8eaf-a14ca676a292",
   "metadata": {},
   "source": [
    "We interpolated CGm data to per-minute interval and normalized by baseline fasting glucose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdc4ab-c523-4cee-ae81-96f8326b88bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from scipy.interpolate import interp1d  # For interpolation\n",
    "\n",
    "\n",
    "def parse_cgm_data(cgm_entry):\n",
    "    if isinstance(cgm_entry, str):\n",
    "        return ast.literal_eval(cgm_entry)  #convert CGM data into a Python literal\n",
    "    return cgm_entry\n",
    "\n",
    "# Function to interpolate CGM data to per-minute values\n",
    "def interpolate_cgm_data(cgm_data):\n",
    "    \"\"\"\n",
    "    Interpolates CGM data to per-minute glucose levels.\n",
    "\n",
    "    Args:\n",
    "        cgm_data (list): Original CGM data as a list of tuples (timestamp, glucose level).\n",
    "\n",
    "    Returns:\n",
    "        list: Interpolated and normalized CGM data as a list of tuples (timestamp as string, glucose level as float).\n",
    "    \"\"\"\n",
    "    # Extract timestamps and glucose levels\n",
    "    timestamps = [entry[0] if isinstance(entry[0], datetime) else datetime.strptime(entry[0], \"%Y-%m-%d %H:%M:%S\") for entry in cgm_data]\n",
    "    glucose_levels = [entry[1] for entry in cgm_data]\n",
    "\n",
    "    baseline_glucose = glucose_levels[0]\n",
    "\n",
    "    # Convert timestamps to seconds since the start for interpolation\n",
    "    seconds = [(t - timestamps[0]).total_seconds() for t in timestamps]\n",
    "\n",
    "    # Interpolation function (linear)\n",
    "    interp_func = interp1d(seconds, glucose_levels, kind=\"linear\")\n",
    "\n",
    "    # Generate per-minute timestamps\n",
    "    min_timestamps = pd.date_range(timestamps[0], timestamps[-1], freq=\"min\")\n",
    "    min_seconds = [(t - timestamps[0]).total_seconds() for t in min_timestamps]\n",
    "\n",
    "    # Interpolated glucose levels\n",
    "    min_glucose_levels = interp_func(min_seconds)\n",
    "\n",
    "    # Normalize glucose levels with the baseline\n",
    "    normalized_glucose_levels = [g / baseline_glucose for g in min_glucose_levels]\n",
    "\n",
    "    # Combine interpolated timestamps and glucose levels with timestamps as strings\n",
    "    interpolated_data = [(t.strftime(\"%Y-%m-%d %H:%M:%S\"), float(g)) for t, g in zip(min_timestamps, normalized_glucose_levels)]\n",
    "    return interpolated_data\n",
    "\n",
    "# Apply preprocessing and interpolation\n",
    "merged_df_wlabels[\"CGM Data\"] = merged_df_wlabels[\"CGM Data\"].apply(parse_cgm_data)  # Parse CGM Data\n",
    "merged_df_wlabels[\"CGM Data Per Minute\"] = merged_df_wlabels[\"CGM Data\"].apply(interpolate_cgm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e3cf5-3a23-4442-b32b-1eb83b1bcbce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(merged_df_wlabels[\"CGM Data\"][2]), len(merged_df_wlabels[\"CGM Data Per Minute\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425c319-94f3-47d3-ac8f-d749d1988bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_wlabels['A1C']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d0da7-f754-4afb-a422-6ca6eb59c7f3",
   "metadata": {},
   "source": [
    "CGM data has variable-length thus in the next step we use padding to ensure all datapoints have length equal to max_lenght=750 using collate_fn function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213564c",
   "metadata": {},
   "source": [
    "## custom Pytorch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8f369-f4ba-473a-a963-b343b35c3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataframe, label_columns, image_columns, cgm_columns, viome_columns, demo_columns, transform=None, target_transform=None):\n",
    "        self.df = dataframe\n",
    "        self.label_columns = label_columns\n",
    "        self.image_columns = image_columns\n",
    "        self.cgm_columns = cgm_columns\n",
    "        self.viome_columns = viome_columns\n",
    "        self.demo_columns = demo_columns\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        # Process image data\n",
    "        image_data = self.df.iloc[idx][self.image_columns[0]]\n",
    "        image = ast.literal_eval(image_data)\n",
    "        image_array = np.array(image)\n",
    "        image_array = image_array / 255 # Normalize the pixel values between 0 and 1\n",
    "        image_tensor = torch.tensor(image_array, dtype=torch.float32).permute(2, 0, 1)  # Shape: (C, H, W)\n",
    "        # Process CGM time-series data (ignore timestamps)\n",
    "        cgm_data = self.df.iloc[idx][self.cgm_columns]\n",
    "        #print(cgm_data)\n",
    "        cgm_values = [entry[1] for entry in cgm_data]\n",
    "        cgm_array = np.array(cgm_values).reshape(-1, 1)\n",
    "        cgm_tensor = torch.tensor(cgm_values, dtype=torch.float32)  # Shape: (time_series_length,)\n",
    "        # adding Viome data\n",
    "        viome_data = self.df.iloc[idx][self.viome_columns[0]]\n",
    "        #print(viome_data)\n",
    "        viome_tensor = torch.tensor(viome_data, dtype=torch.float32)\n",
    "        demo_data = self.df.iloc[idx][self.demo_columns]\n",
    "        #print(f\"Demo data at index {idx}: {demo_data}\") \n",
    "        demo_tensor = torch.tensor(demo_data, dtype=torch.float32)\n",
    "        # adding label\n",
    "        label = self.df.iloc[idx][self.label_columns].values[0]\n",
    "        #print(label)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "        if self.target_transform:\n",
    "            label_tensor = self.target_transform(label_tensor)\n",
    "        return image_tensor, cgm_tensor, viome_tensor, demo_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21f4f5-04dc-4a1e-8fc1-db144abaf460",
   "metadata": {},
   "source": [
    "- Lunch calories are selected as our label.\n",
    "- Image before lunch, Viome, CGM data per minute and anthropometric data are selected as features.\n",
    "- For anthropometric data, we excluded some features which are highly correlated with ones in the current list to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d95ec-6bb2-4f1c-b674-cc31b82786d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Lunch Calories']\n",
    "image_columns = ['Image Before Lunch']\n",
    "viome_columns = ['Viome']\n",
    "demo_columns = ['A1C', 'Insulin', 'Triglycerides', 'Cholesterol','HDL', 'Non-HDL', 'VLDL', 'HOMA-IR', 'BMI']\n",
    "cgm_columns = 'CGM Data Per Minute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MultiModalDataset\n",
    "dataset = MultiModalDataset(\n",
    "    dataframe=merged_df_wlabels,  \n",
    "    label_columns=label_columns,\n",
    "    image_columns=image_columns,\n",
    "    cgm_columns=cgm_columns,  \n",
    "    viome_columns=viome_columns,\n",
    "    demo_columns = demo_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Custom collate function for handling variable-length sequences (since CGM data have variable length)\n",
    "def collate_fn(batch):\n",
    "    images, cgm_tensors, viomes, demos, labels = zip(*batch)\n",
    "\n",
    "    # Stack image tensors and labels\n",
    "    images = torch.stack(images)  # Shape: (batch_size, C, H, W)\n",
    "    labels = torch.stack(labels)  # Shape: (batch_size,)\n",
    "    viomes = torch.stack(viomes)\n",
    "    demos = torch.stack(demos)\n",
    "\n",
    "    # Pad CGM tensors to the max_length, # Shape: (batch_size, max_seq_len)\n",
    "    max_length = 750\n",
    "    cgm_tensors = pad_sequence([F.pad(tensor, (0, max_length - len(tensor)), \"constant\", 0)\n",
    "                               for tensor in cgm_tensors], batch_first=True)\n",
    "\n",
    "    # Reshape CGM tensor to add an 'input_size' dimension (1 for single feature per time-step)\n",
    "    cgm_tensors = cgm_tensors.unsqueeze(-1)  # Shape: (batch_size, max_seq_len, 1)\n",
    "    viomes = viomes.unsqueeze(-1)\n",
    "\n",
    "    demos = demos.unsqueeze(-1)\n",
    "    return images, cgm_tensors, viomes, demos, labels\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for train and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827e8bf-d80e-40af-9bf7-763181d39ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "    print(f\"Training Batch {batch_idx + 1}:\")\n",
    "    print(f\"Image batch shape: {images.shape}\")  # (batch_size, num_images, C, H, W)\n",
    "    print(f\"CGM batch shape: {cgm_tensors.shape}\")  # (batch_size, max_seq_len)\n",
    "    print(f\"Viome batch shape: {viomes.shape}\")  # (batch_size, max_seq_len)\n",
    "    print(f\"Demo batch shape: {demos.shape}\")  # (batch_size, max_seq_len)\n",
    "    print(f\"Labels shape: {labels.shape}\")  # (batch_size,)\n",
    "    print(demos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20afe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to plot an image, CGM and Viome data \n",
    "def plot_image_with_label(image_tensor, label, cgm_tensor, viomes, batch_index):\n",
    "    # Convert tensor from C x H x W to H x W x C for plotting\n",
    "    image = image_tensor.permute(1, 2, 0).numpy()\n",
    "    image = image * 255\n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.astype(np.uint8))\n",
    "    plt.title(f'Batch {batch_index + 1} - Label: {label.item()}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot the CGM data\n",
    "    cgm_values = cgm_tensor.numpy()\n",
    "    valid_cgm_values = cgm_values\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(valid_cgm_values)\n",
    "    plt.title('CGM Time-Series Data')\n",
    "    plt.xlabel('Time Points')\n",
    "    plt.ylabel('CGM Value')\n",
    "\n",
    "    # Plot the viome data\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(viomes)\n",
    "    plt.title('Viome Data')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iterate through the train DataLoader and plot one image, CGM, and viome data per batch\n",
    "for batch_idx, (images, cgm_tensors, viomes,demos, labels) in enumerate(train_loader):\n",
    "    print(f\"Training Batch {batch_idx + 1}:\")\n",
    "    print(f\"Image batch shape: {images.shape}\")  # (batch_size, num_images, C, H, W)\n",
    "    print(f\"CGM batch shape: {cgm_tensors.shape}\")  # (batch_size, max_seq_len)\n",
    "    print(f\"Viome batch shape: {viomes.shape}\")  # (batch_size, max_seq_len)\n",
    "    print(f\"Labels shape: {labels.shape}\")  # (batch_size,)\n",
    "\n",
    "    # Plot the first image, CGM and viome data in the batch\n",
    "    first_image = images[0] # First image of the first sample\n",
    "    first_cgm = cgm_tensors[0]  # First CGM time-series data\n",
    "    first_label = labels[0]\n",
    "    first_viome = viomes[0] # First Viome data\n",
    "\n",
    "    plot_image_with_label(first_image, first_label, first_cgm, first_viome, batch_idx)\n",
    "    break  # Stop after first batch for visualization\n",
    "\n",
    "# Iterate through the valid DataLoader to do the same\n",
    "for batch_idx, (images, cgm_tensors, viomes, demos,labels) in enumerate(val_loader):\n",
    "    print(f\"Validation Batch {batch_idx + 1}:\")\n",
    "    print(f\"Image batch shape: {images.shape}\")  # (batch_size, num_images, C, H, W)\n",
    "    print(f\"CGM batch shape: {cgm_tensors.shape}\")  # (batch_size, max_seq_len)\n",
    "    print(f\"Viome batch shape: {viomes.shape}\")  # (batch_size, max_seq_len)\n",
    "    print(f\"Labels shape: {labels.shape}\")  # (batch_size,)\n",
    "\n",
    "    first_image = images[0]\n",
    "    first_cgm = cgm_tensors[0]\n",
    "    first_label = labels[0]\n",
    "    first_viome = viomes[0]\n",
    "\n",
    "    plot_image_with_label(first_image, first_label, first_cgm, first_viome, batch_idx)\n",
    "    break  # Stop after first batch for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3ef7c",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e6df4-1079-41d0-b6b3-8c21b71b9a81",
   "metadata": {},
   "source": [
    "- We initially started with relatively simpler model. Using combination of CNN and LSTM for image and CGM data respectively to extract embedding from them.\n",
    "-  CNN: we apply two convolutional layers (with 32 and 64 filters of 3x3 size) with ReLU activation and 2x2 max pooling and final fully connected layer to get a set of 64 embeddings from the image.\n",
    "- We use a two-layer LSTM with 64 units, which outputs 64 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d063d-747e-413f-8a53-ff72f7af1c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, embedding_size=64):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        embedding = self.fc(last_hidden_state)\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, embedding_size=64):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 max pooling\n",
    "        self.fc = nn.Linear(64 * 16 * 16, embedding_size)  # After 2 pooling layers, output size = 64x64 -> 16x16\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output (batch_size, 64*16*16)\n",
    "        embedding = self.fc(x)  # Final 64-dimensional embedding\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a7cdb-e43d-4caa-8ce7-d83b23f920a0",
   "metadata": {},
   "source": [
    "- We use MultiModalModel function to concatenate embeddings into joint embeddings and then fed into Regression model that is final neural network that we use to get final prediction of lunch calorie.\n",
    "- References for this part of the code: https://github.com/stmilab/joint_embedding_calorie_prediction/tree/main?tab=readme-ov-file\n",
    "- Final network contains two fully connected layer with ReLU activation and dropout, followed by final FC layer that gives the output prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af494512-7970-4e46-a62f-b36f6f898840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, models, n_classes=1):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        zs = [model(x) for model, x in zip(self.models, xs)]\n",
    "        return zs\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, input_size=64, hidden=128, output_size=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac7d2f",
   "metadata": {},
   "source": [
    "### Image (CNN) + CGM (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31538f-c122-4f8c-8edb-6a485b7ba649",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_model = LSTM(input_size=1, hidden_size=64, num_layers=2, embedding_size=64)\n",
    "img_model = CNN(embedding_size=64)\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Combining the multimodal model\n",
    "multimodal_model = MultiModalModel([img_model, cgm_model,]).to(device)\n",
    "\n",
    "#prediction model\n",
    "expected_input_size = 64 + 64\n",
    "nutrient_predictor = Regressor(expected_input_size, hidden=128, output_size=1, dropout=0.2).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters()),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "# Initialize lists to store epoch losses\n",
    "m1_train_losses = []\n",
    "m1_val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, cgm_tensors, demos, labels = images.to(device), cgm_tensors.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        img_embedding, cgm_embedding = multimodal_model([images, cgm_tensors])\n",
    "        combined_embedding = torch.cat((img_embedding, cgm_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "        \n",
    "        # Calculate RMSRE Loss\n",
    "        msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "        msre.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss.append(msre.item())\n",
    "\n",
    "    avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "    m1_train_losses.append(avg_train_loss)  # Store train loss\n",
    "    \n",
    "    # Validation phase\n",
    "    epoch_val_loss = []\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, cgm_tensors, viomes, demos, labels in val_loader:\n",
    "            images, cgm_tensors, demos, labels = images.to(device), cgm_tensors.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "            \n",
    "            img_embedding, cgm_embedding = multimodal_model([images, cgm_tensors])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "            \n",
    "            val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss))\n",
    "        m1_val_losses.append(avg_val_rmsre)  # Store validation loss\n",
    "        \n",
    "        # Track minimum loss\n",
    "        if avg_val_rmsre < min_loss:\n",
    "            min_loss = avg_val_rmsre\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17677e7-4c74-41f3-b94a-f891ad76ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train and Validation Loss\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(range(epochs), m1_train_losses, label='Train', marker='o', color='b')\n",
    "plt.plot(range(epochs), m1_val_losses, label='Validation', marker='s',color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSRE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(m1_train_losses[14:]),np.std(m1_val_losses[14:]))\n",
    "print(np.mean(m1_val_losses[14:]),np.std(m1_val_losses[14:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bd3cb-e3ba-4eb7-892c-da13e5d668f2",
   "metadata": {},
   "source": [
    "- Next, we turn to more complex models.\n",
    "- We use vision transformers for Image data and transformer encoder for CGM data as in Zhang et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b20ec",
   "metadata": {},
   "source": [
    "### Image + CGM (transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378787d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        num_classes,\n",
    "        dropout=0,\n",
    "        num_layers=6,\n",
    "        use_pos_emb=False,\n",
    "        activation=nn.GELU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_pos_emb = use_pos_emb\n",
    "\n",
    "        self.conv = nn.Conv1d(n_features, embed_dim, 1, 1)\n",
    "        self.attn = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "            ),\n",
    "            num_layers,\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"position_vec\",\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    math.pow(10000.0, 2.0 * (i // 2) / embed_dim)\n",
    "                    for i in range(embed_dim)\n",
    "                ],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(embed_dim, num_classes)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def temporal_enc(self, time, non_pad_mask):\n",
    "        \"\"\"\n",
    "        Input: batch*seq_len.\n",
    "        Output: batch*seq_len*d_model.\n",
    "        \"\"\"\n",
    "\n",
    "        result = time[:, None] / self.position_vec[None, :, None]\n",
    "        result[:, :, 0::2] = torch.sin(result[:, :, 0::2])\n",
    "        result[:, :, 1::2] = torch.cos(result[:, :, 1::2])\n",
    "        return result * non_pad_mask[:, None]\n",
    "\n",
    "    def forward(self, x, lens=0, t=0):\n",
    "        mask = (x == float(\"inf\"))[:, :, 0]\n",
    "        x[mask] = 0\n",
    "\n",
    "        z = self.conv(x.permute(0, 2, 1))\n",
    "\n",
    "        if self.use_pos_emb:\n",
    "            tem_enc = self.temporal_enc(t, mask)\n",
    "            z = z + tem_enc\n",
    "\n",
    "        z = z.permute(0, 2, 1).float()\n",
    "\n",
    "        z = self.attn(z, src_key_padding_mask=mask)\n",
    "\n",
    "        return self.linear(z.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "import math\n",
    "\n",
    "\n",
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)  # [B, H', W`, C, p_H, p_W]\n",
    "    x = x.flatten(1, 2)  # [B, H`*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2, 4)  # [B, H'*W', C*p_H*p_W]\n",
    "    return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_channels,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        num_classes,\n",
    "        patch_size,\n",
    "        num_patches,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels * (patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:, : T + 1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6f07c-d148-4507-959f-f914ecc26e0f",
   "metadata": {},
   "source": [
    "- We also used transformer encode we used for CGM data for Viome and anthropometric data as well.\n",
    "- We also compared the performance of transformer encoder against simple MLP (class called FC) for Viome and anthropometric data\n",
    "- Next, we assessed the performance different combinations of joint modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for CGM model\n",
    "cgm_model_kwargs = {\n",
    "    'n_features': 1,\n",
    "    'embed_dim': 96,\n",
    "    'num_heads': 4,\n",
    "    'num_classes': 64,\n",
    "    'dropout': 0.2,\n",
    "    'num_layers': 6\n",
    "}\n",
    "\n",
    "cgm_model = MultiheadAttention(**cgm_model_kwargs).to(device)\n",
    "\n",
    "# Define parameters for Vision Transformer model\n",
    "img_model_kwargs = {\n",
    "    'embed_dim': 64,\n",
    "    'hidden_dim': 128,\n",
    "    'num_channels': 3,\n",
    "    'num_heads': 2,\n",
    "    'num_layers': 6,\n",
    "    'num_classes': 64,\n",
    "    'patch_size': 4,\n",
    "    'num_patches': 256,\n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "img_model = VisionTransformer(**img_model_kwargs).to(device)\n",
    "\n",
    "# Define parameters for viome model\n",
    "viome_model_kwargs = {\n",
    "    'n_features': 1,\n",
    "    'embed_dim': 96,\n",
    "    'num_heads': 4,\n",
    "    'num_classes': 8,\n",
    "    'dropout': 0.2,\n",
    "    'num_layers': 6\n",
    "}\n",
    "\n",
    "viome_model = MultiheadAttention(**viome_model_kwargs).to(device)\n",
    "\n",
    "# Define parameters for demo model\n",
    "demo_model_kwargs = {\n",
    "    'n_features': 1,\n",
    "    'embed_dim': 96,\n",
    "    'num_heads': 4,\n",
    "    'num_classes': 8,\n",
    "    'dropout': 0.2,\n",
    "    'num_layers': 6\n",
    "}\n",
    "\n",
    "demo_model = MultiheadAttention(**demo_model_kwargs).to(device)\n",
    "\n",
    "\n",
    "# Combining the multimodal model\n",
    "multimodal_model = MultiModalModel([img_model, cgm_model],).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction model\n",
    "\n",
    "expected_input_size = 64 + 64\n",
    "nutrient_predictor = Regressor(expected_input_size).to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters()),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize lists to store epoch losses\n",
    "m2_train_losses = []\n",
    "m2_val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, cgm_tensors, demos, labels = images.to(device), cgm_tensors.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        img_embedding, cgm_embedding = multimodal_model([images, cgm_tensors])\n",
    "        combined_embedding = torch.cat((img_embedding, cgm_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "        \n",
    "        # Calculate RMSRE Loss\n",
    "        msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "        msre.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss.append(msre.item())\n",
    "\n",
    "    avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "    m2_train_losses.append(avg_train_loss)  # Store train loss\n",
    "    \n",
    "    # Validation phase\n",
    "    epoch_val_loss = []\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, cgm_tensors, viomes, demos, labels in val_loader:\n",
    "            images, cgm_tensors, demos, labels = images.to(device), cgm_tensors.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "            \n",
    "            img_embedding, cgm_embedding = multimodal_model([images, cgm_tensors])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "            \n",
    "            val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss))\n",
    "        m2_val_losses.append(avg_val_rmsre)  # Store validation loss\n",
    "        \n",
    "        # Track minimum loss\n",
    "        if avg_val_rmsre < min_loss:\n",
    "            min_loss = avg_val_rmsre\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train and Validation Loss\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(range(epochs), m2_train_losses, label='Train', marker='o', color='b')\n",
    "plt.plot(range(epochs), m2_val_losses, label='Validation', marker='s',color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSRE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24735b38-c4dc-4508-9cfa-f2ec6fb4e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(m2_train_losses[14:]),np.std(m2_train_losses[14:]))\n",
    "print(np.mean(m2_val_losses[14:]),np.std(m2_val_losses[14:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634a239",
   "metadata": {},
   "source": [
    "### Image+Demo (transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the multimodal model\n",
    "multimodal_model = MultiModalModel([img_model, demo_model],).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction model\n",
    "\n",
    "expected_input_size = 64 + 8\n",
    "nutrient_predictor = Regressor(expected_input_size).to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(demo_model.parameters()),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize lists to store epoch losses\n",
    "m3_train_losses = []\n",
    "m3_val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, cgm_tensors, demos, labels = images.to(device), cgm_tensors.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        img_embedding, demo_embedding = multimodal_model([images, demos])\n",
    "        combined_embedding = torch.cat((img_embedding, demo_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "        \n",
    "        # Calculate RMSRE Loss\n",
    "        msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "        msre.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss.append(msre.item())\n",
    "\n",
    "    avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "    m3_train_losses.append(avg_train_loss)  # Store train loss\n",
    "    \n",
    "    # Validation phase\n",
    "    epoch_val_loss = []\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, cgm_tensors, viomes, demos, labels in val_loader:\n",
    "            images, cgm_tensors, demos, labels = images.to(device), cgm_tensors.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "            \n",
    "            img_embedding, demo_embedding = multimodal_model([images, demos])\n",
    "            combined_embedding = torch.cat((img_embedding, demo_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "            \n",
    "            val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss))\n",
    "        m3_val_losses.append(avg_val_rmsre)  # Store validation loss\n",
    "        \n",
    "        # Track minimum loss\n",
    "        if avg_val_rmsre < min_loss:\n",
    "            min_loss = avg_val_rmsre\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train and Validation Loss\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(range(epochs), m3_train_losses, label='Train', marker='o', color='b')\n",
    "plt.plot(range(epochs), m3_val_losses, label='Validation', marker='s',color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSRE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(m3_train_losses[14:]),np.std(m3_train_losses[14:]))\n",
    "print(np.mean(m3_val_losses[14:]),np.std(m3_val_losses[14:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ab337",
   "metadata": {},
   "source": [
    "### Image+Viome (transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c83896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the multimodal model\n",
    "multimodal_model = MultiModalModel([img_model, viome_model],).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction model\n",
    "\n",
    "expected_input_size = 64 + 8\n",
    "nutrient_predictor = Regressor(expected_input_size).to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(viome_model.parameters()),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize lists to store epoch losses\n",
    "m4_train_losses = []\n",
    "m4_val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, cgm_tensors,viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        img_embedding, viome_embedding = multimodal_model([images, viomes])\n",
    "        combined_embedding = torch.cat((img_embedding, viome_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "        \n",
    "        # Calculate RMSRE Loss\n",
    "        msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "        msre.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss.append(msre.item())\n",
    "\n",
    "    avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "    m4_train_losses.append(avg_train_loss)  # Store train loss\n",
    "    \n",
    "    # Validation phase\n",
    "    epoch_val_loss = []\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, cgm_tensors, viomes, demos, labels in val_loader:\n",
    "            images, cgm_tensors,viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "            \n",
    "            img_embedding, viome_embedding = multimodal_model([images, viomes])\n",
    "            combined_embedding = torch.cat((img_embedding, viome_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "            \n",
    "            val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss))\n",
    "        m4_val_losses.append(avg_val_rmsre)  # Store validation loss\n",
    "        \n",
    "        # Track minimum loss\n",
    "        if avg_val_rmsre < min_loss:\n",
    "            min_loss = avg_val_rmsre\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a567913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train and Validation Loss\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(range(epochs), m4_train_losses, label='Train', marker='o', color='b')\n",
    "plt.plot(range(epochs), m4_val_losses, label='Validation', marker='s',color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSRE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(m4_train_losses[14:]),np.std(m4_train_losses[14:]))\n",
    "print(np.mean(m4_val_losses[14:]),np.std(m4_val_losses[14:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9cd556-a0ba-4ca4-a0fb-b7a12a88c752",
   "metadata": {},
   "source": [
    "### Image + CGM + Demo (transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6e17c-4341-455a-b371-d494dc260e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for demo model\n",
    "# demo_model_kwargs = {\n",
    "#     'n_features': 1,\n",
    "#     'embed_dim': 96,\n",
    "#     'num_heads': 4,\n",
    "#     'num_classes': 8,\n",
    "#     'dropout': 0.2,\n",
    "#     'num_layers': 6\n",
    "# }\n",
    "\n",
    "# demo_model = MultiheadAttention(**demo_model_kwargs).to(device)\n",
    "\n",
    "# Combining the multimodal model\n",
    "multimodal_model = MultiModalModel([img_model, cgm_model, demo_model],).to(device)\n",
    "\n",
    "#prediction model\n",
    "expected_input_size = 64 + 64 + 8\n",
    "nutrient_predictor = Regressor(expected_input_size, hidden=128, output_size=1, dropout=0.2).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters())\n",
    "        + list(demo_model.parameters()),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize lists to store epoch losses\n",
    "m5_train_losses = []\n",
    "m5_val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "\n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, cgm_tensors, demos, labels = images.to(device), cgm_tensors.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        img_embedding, cgm_embedding, demo_embedding = multimodal_model([images, cgm_tensors, demos])\n",
    "        combined_embedding = torch.cat((img_embedding, cgm_embedding, demo_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "        \n",
    "        # Calculate RMSRE Loss\n",
    "        msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "        msre.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss.append(msre.item())\n",
    "\n",
    "    avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "    m5_train_losses.append(avg_train_loss)  # Store train loss\n",
    "    \n",
    "    # Validation phase\n",
    "    epoch_val_loss = []\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, cgm_tensors, viomes, demos, labels in val_loader:\n",
    "            images, cgm_tensors, demos, labels = images.to(device), cgm_tensors.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "            \n",
    "            img_embedding, cgm_embedding, demo_embedding = multimodal_model([images, cgm_tensors, demos])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding, demo_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "            \n",
    "            val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss))\n",
    "        m5_val_losses.append(avg_val_rmsre)  # Store validation loss\n",
    "        \n",
    "        # Track minimum loss\n",
    "        if avg_val_rmsre < min_loss:\n",
    "            min_loss = avg_val_rmsre\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd45d97-88bf-400a-a362-8f4dd745cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train and Validation Loss\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(range(epochs), m5_train_losses, label='Train', marker='o', color='b')\n",
    "plt.plot(range(epochs), m5_val_losses, label='Validation', marker='s',color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSRE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(np.mean(m5_train_losses[14:]),np.std(m5_train_losses[14:]))\n",
    "print(np.mean(m5_val_losses[14:]),np.std(m5_val_losses[14:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e8431-15ca-48bd-929f-8d3617854474",
   "metadata": {},
   "source": [
    "### Image+CGM+Viome (transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb332d8-34ed-49ed-a450-451eddd1d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self, input_size=27, embedding_size=27):\n",
    "        super(FC, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),                \n",
    "            nn.Linear(input_size, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embedding_size)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.fc(x)  \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c099332-736f-4846-8468-10b77f2ec90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define parameters for Viome model\n",
    "# viome_model_kwargs = {\n",
    "#     'n_features': 1,\n",
    "#     'embed_dim': 96,\n",
    "#     'num_heads': 4,\n",
    "#     'num_classes': 8,\n",
    "#     'dropout': 0.2,\n",
    "#     'num_layers': 6\n",
    "# }\n",
    "\n",
    "# # Create viome model\n",
    "# viome_model =  MultiheadAttention(**viome_model_kwargs).to(device)\n",
    "# #viome_model = FC(input_size=27, embedding_size=27).to(device)\n",
    "\n",
    "# Combining the multimodal model\n",
    "multimodal_model = MultiModalModel([img_model, cgm_model, viome_model],).to(device)\n",
    "\n",
    "#prediction model\n",
    "expected_input_size = 64 + 64 + 8\n",
    "nutrient_predictor = Regressor(expected_input_size, hidden=128, output_size=1, dropout=0.2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters())\n",
    "        + list(viome_model.parameters()),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize lists to store epoch losses\n",
    "m6_train_losses = []\n",
    "m6_val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "\n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, cgm_tensors, viomes, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), labels.to(device).unsqueeze(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        img_embedding, cgm_embedding, viome_embedding = multimodal_model([images, cgm_tensors, viomes])\n",
    "        combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "        \n",
    "        # Calculate RMSRE Loss\n",
    "        msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "        msre.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss.append(msre.item())\n",
    "\n",
    "    avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "    m6_train_losses.append(avg_train_loss)  # Store train loss\n",
    "    \n",
    "    # Validation phase\n",
    "    epoch_val_loss = []\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(val_loader):\n",
    "            images, cgm_tensors, viomes, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), labels.to(device).unsqueeze(-1)\n",
    "            \n",
    "            img_embedding, cgm_embedding, viome_embedding = multimodal_model([images, cgm_tensors, viomes])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "            \n",
    "            val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss))\n",
    "        m6_val_losses.append(avg_val_rmsre)  # Store validation loss\n",
    "        \n",
    "        # Track minimum loss\n",
    "        if avg_val_rmsre < min_loss:\n",
    "            min_loss = avg_val_rmsre\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d5d20-c0c8-4a01-a766-b5ad21b8e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train and Validation Loss\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(range(epochs), m6_train_losses, label='Train', marker='o', color='b')\n",
    "plt.plot(range(epochs), m6_val_losses, label='Validation', marker='s',color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSRE')\n",
    "plt.legend()\n",
    "plt.savefig('jointmodel.png', dpi=300)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(m6_train_losses[14:]),np.std(m6_train_losses[14:]))\n",
    "print(np.mean(m6_val_losses[14:]),np.std(m6_val_losses[14:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1be024-70f0-4018-8474-b8645bd020a0",
   "metadata": {},
   "source": [
    "# Model adding all modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0cc05-a3cc-4097-8a9a-6e2671fac0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the multimodal model\n",
    "multimodal_model = MultiModalModel([img_model, cgm_model, viome_model, demo_model],).to(device)\n",
    "\n",
    "#prediction model\n",
    "expected_input_size = 64 + 64 + 8 + 8\n",
    "nutrient_predictor = Regressor(expected_input_size, hidden=128, output_size=1, dropout=0.2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters())\n",
    "        + list(viome_model.parameters())\n",
    "        + list(demo_model.parameters()),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize lists to store epoch losses\n",
    "m7_train_losses = []\n",
    "m7_val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "\n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "        combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "        \n",
    "        # Calculate RMSRE Loss\n",
    "        msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "        msre.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss.append(msre.item())\n",
    "\n",
    "    avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "    m7_train_losses.append(avg_train_loss)  # Store train loss\n",
    "    \n",
    "    # Validation phase\n",
    "    epoch_val_loss = []\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(val_loader):\n",
    "            images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "            \n",
    "            img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "            \n",
    "            val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss))\n",
    "        m7_val_losses.append(avg_val_rmsre)  # Store validation loss\n",
    "        \n",
    "        # Track minimum loss\n",
    "        if avg_val_rmsre < min_loss:\n",
    "            min_loss = avg_val_rmsre\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458e6ed-4221-4227-abac-3fc57a6cc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train and Validation Loss\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(range(epochs), m7_train_losses, label='Train', marker='o', color='b')\n",
    "plt.plot(range(epochs), m7_val_losses, label='Validation', marker='s',color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSRE')\n",
    "plt.legend()\n",
    "plt.savefig('jointmodel.png', dpi=300)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(m7_train_losses[1:]),np.std(m7_train_losses[1:]))\n",
    "print(np.mean(m7_val_losses[1:]),np.std(m7_val_losses[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42f9fa-ed62-4236-b6cc-37eabef616eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(m7_train_losses[14:]),np.std(m7_train_losses[14:]))\n",
    "print(np.mean(m7_val_losses[14:]),np.std(m7_val_losses[14:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12773315",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with all modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b3690-1b93-40af-82a2-e376913a0f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6267f23-0b9a-4482-99ba-b07950e2c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15 #since RMSE is plateauing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b9efa-49b3-47fe-b6de-93b6d13b2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr and weight_decay in optimizer\n",
    "# dropout rate and number of hidden in Regressor model\n",
    "# batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cfa1d-5cdc-4f8e-a3ce-f9fdbde748b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Learning rates to test\n",
    "learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "# Initialize the best loss and model state\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_params = None\n",
    "best_lr = None\n",
    "\n",
    "expected_input_size = 64 + 64 + 8 + 8\n",
    "\n",
    "# Loop through each learning rate\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "#     multimodal_model = MultiModalModel([img_model, cgm_model, viome_model]).to(device)\n",
    "    multimodal_model = MultiModalModel([img_model, cgm_model, viome_model, demo_model],).to(device)\n",
    "    nutrient_predictor = Regressor(expected_input_size, hidden=128, output_size=1, dropout=0.2).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters())\n",
    "        + list(viome_model.parameters())\n",
    "        + list(demo_model.parameters()),\n",
    "        lr=lr,  # changing lr\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "    # Initialize lists to store epoch losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = []\n",
    "        \n",
    "        # Training phase\n",
    "        multimodal_model.train()\n",
    "        for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "            # Calculate RMSRE Loss\n",
    "            msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "            msre.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss.append(msre.item())\n",
    "\n",
    "        avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_loss = []\n",
    "        multimodal_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(val_loader):\n",
    "                images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "                img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "                combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "                predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "                val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "                epoch_val_loss.append(val_loss.item())\n",
    "\n",
    "        # Compute average validation loss across the entire epoch\n",
    "        avg_val_loss = np.sqrt(np.mean(epoch_val_loss))\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.3f}, Val Loss: {avg_val_loss:.3f}\")\n",
    "        \n",
    "    # select the best model based on average validation loss\n",
    "    average_val_loss = np.mean(val_losses[-10:])\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"\\nBest Learning Rate: {best_lr}, Best Validation Loss: {best_val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb2f9d-dd6f-49e3-b467-509f44a76daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5098bfb-a528-4c4f-883f-dd59f1964beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight decays to test\n",
    "weight_decays = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Initialize the best loss and model state\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_params = None\n",
    "best_decay = None\n",
    "\n",
    "# Loop through each weight decay\n",
    "for weight_decay in weight_decays:\n",
    "    print(f\"\\nTesting weight decay: {weight_decay}\")\n",
    "    multimodal_model = MultiModalModel([img_model, cgm_model, viome_model, demo_model],).to(device)\n",
    "    nutrient_predictor = Regressor(expected_input_size, hidden=128, output_size=1, dropout=0.2).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters())\n",
    "        + list(viome_model.parameters())\n",
    "        + list(demo_model.parameters()),\n",
    "        lr=best_lr,  # changing lr\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Initialize lists to store epoch losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = []\n",
    "\n",
    "        # Training phase\n",
    "        multimodal_model.train()\n",
    "        for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "            # Calculate RMSRE Loss\n",
    "            msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "            msre.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss.append(msre.item())\n",
    "\n",
    "        avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_loss = []\n",
    "        multimodal_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(val_loader):\n",
    "                images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "                img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "                combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "                predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "                val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "                epoch_val_loss.append(val_loss.item())\n",
    "\n",
    "        # Compute average validation loss across the entire epoch\n",
    "        avg_val_loss = np.sqrt(np.mean(epoch_val_loss))\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Compute average validation loss across the entire epoch\n",
    "        avg_val_loss = np.sqrt(np.mean(epoch_val_loss))\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.3f}, Val Loss: {avg_val_loss:.3f}\")\n",
    "    \n",
    "    # select the best model based on average validation loss\n",
    "    average_val_loss = np.mean(val_losses[-10:])\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_decay = weight_decay\n",
    "\n",
    "print(f\"\\nBest Weight Decay: {best_decay}, Best Validation Loss: {best_val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6212bae-0e1c-4dd0-bbbe-0e4280639e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of hidden units to test\n",
    "hidden_units = [32, 64, 128, 256]\n",
    "\n",
    "# Initialize the best loss and model state\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_params = None\n",
    "best_hidden = None\n",
    "\n",
    "# Loop through each number of hidden units\n",
    "for hidden in hidden_units:\n",
    "    print(f\"\\nTesting # hidden units: {hidden}\")\n",
    "    multimodal_model = MultiModalModel([img_model, cgm_model, viome_model, demo_model],).to(device)\n",
    "    nutrient_predictor = Regressor(expected_input_size, hidden=hidden, output_size=1, dropout=0.2).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters())\n",
    "        + list(viome_model.parameters())\n",
    "        + list(demo_model.parameters()),\n",
    "        lr=best_lr,  # changing lr\n",
    "        weight_decay=best_decay,\n",
    "    )\n",
    "\n",
    "    # Initialize lists to store epoch losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = []\n",
    "\n",
    "        # Training phase\n",
    "        multimodal_model.train()\n",
    "        for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "            # Calculate RMSRE Loss\n",
    "            msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "            msre.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss.append(msre.item())\n",
    "\n",
    "        avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_loss = []\n",
    "        multimodal_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(val_loader):\n",
    "                images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "                img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "                combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "                predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "                val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "                epoch_val_loss.append(val_loss.item())\n",
    "\n",
    "        # Compute average validation loss across the entire epoch\n",
    "        avg_val_loss = np.sqrt(np.mean(epoch_val_loss))\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.3f}, Val Loss: {avg_val_loss:.3f}\")\n",
    "    \n",
    "    # select the best model based on average validation loss\n",
    "    average_val_loss = np.mean(val_losses[-10:])\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_hidden = hidden\n",
    "\n",
    "print(f\"\\nBest # hidden units: {best_hidden}, Best Validation Loss: {best_val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563057d9-fc4d-4934-849f-25027e8d01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout rates to test\n",
    "dropout_rates = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Initialize the best loss and model state\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_params = None\n",
    "best_drop = None\n",
    "\n",
    "# Loop through each dropout rate\n",
    "for drop in dropout_rates:\n",
    "    print(f\"\\nTesting dropout rate: {drop}\")\n",
    "    multimodal_model = MultiModalModel([img_model, cgm_model, viome_model, demo_model],).to(device)\n",
    "    nutrient_predictor = Regressor(expected_input_size, hidden=hidden, output_size=1, dropout=drop).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters())\n",
    "        + list(viome_model.parameters())\n",
    "        + list(demo_model.parameters()),\n",
    "        lr=best_lr,  # changing lr\n",
    "        weight_decay=best_decay,\n",
    "    )\n",
    "    \n",
    "    # Initialize lists to store losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = []\n",
    "\n",
    "        # Training phase\n",
    "        multimodal_model.train()\n",
    "        for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "            # Calculate RMSRE Loss\n",
    "            msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "            msre.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss.append(msre.item())\n",
    "\n",
    "        avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        epoch_val_loss = []\n",
    "        multimodal_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(val_loader):\n",
    "                images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "                img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "                combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "                predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "                val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "                epoch_val_loss.append(val_loss.item())\n",
    "\n",
    "        # Compute average validation loss (RMSRE)\n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss[1:]))\n",
    "        val_losses.append(avg_val_rmsre)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")\n",
    "\n",
    "    # select the best model based on average validation loss\n",
    "    average_val_loss = np.mean(val_losses[-10:])\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_drop = drop\n",
    "        \n",
    "print(f\"\\nBest dropout rate: {best_drop}, Best Validation RMSRE: {best_val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7fd23-13dc-446c-a998-0013c2324302",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best learning rate:',best_lr)\n",
    "print('Best Weight decay:',best_decay)\n",
    "print('Best number of hidden layer:',best_hidden)\n",
    "print('Best dropout rate:',best_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e23cb2-d777-4f9e-800c-98af0cc005ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the multimodal model\n",
    "multimodal_model = MultiModalModel([img_model, cgm_model, viome_model, demo_model],).to(device)\n",
    "\n",
    "#prediction model\n",
    "expected_input_size = 64 + 64 + 8 + 8\n",
    "nutrient_predictor = Regressor(expected_input_size, hidden=64, output_size=1, dropout=best_drop).to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "        list(nutrient_predictor.parameters())\n",
    "        + list(img_model.parameters())\n",
    "        + list(cgm_model.parameters())\n",
    "        + list(viome_model.parameters())\n",
    "        + list(demo_model.parameters()),\n",
    "        lr=best_lr,  # changing lr\n",
    "        weight_decay=best_decay,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize lists to store epoch losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "min_loss = float(\"inf\")\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    # Training phase\n",
    "    multimodal_model.train()\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "        # Forward pass\n",
    "        img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "        combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "        # Calculate RMSRE Loss\n",
    "        msre = torch.mean((predictions - labels) ** 2 / (labels ** 2))\n",
    "        msre.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss.append(msre.item())\n",
    "\n",
    "    avg_train_loss = np.sqrt(np.mean(epoch_train_loss))\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    epoch_val_loss = []\n",
    "    multimodal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, cgm_tensors, viomes, demos, labels) in enumerate(val_loader):\n",
    "            images, cgm_tensors, viomes, demos, labels = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "            img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "            combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "            predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "            val_loss = torch.mean(((labels - predictions) / labels) ** 2)\n",
    "            epoch_val_loss.append(val_loss.item())\n",
    "        \n",
    "        avg_val_rmsre = np.sqrt(np.mean(epoch_val_loss))\n",
    "        val_losses.append(avg_val_rmsre)  # Store validation loss\n",
    "        \n",
    "        # Track minimum loss\n",
    "        if avg_val_rmsre < min_loss:\n",
    "            min_loss = avg_val_rmsre\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.3f}, Val RMSRE: {avg_val_rmsre:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d975737-6243-4c68-8562-8aeccdaa7f40",
   "metadata": {},
   "source": [
    "## Processing test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba39b6-fe05-4f51-86a2-995f0fd9cd87",
   "metadata": {},
   "source": [
    "### Viome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d845b56-a280-4d9a-b9d2-1c0989461dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_viome_test = pd.read_csv('demo_viome_test.csv')\n",
    "demo_viome_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd28d3-4599-4df5-9036-9e2e11592050",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_viome_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c4e65-93ca-4b86-ab76-ea897b1dca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_viome_test_processed = demo_viome_data_preprocessing(demo_viome_test)\n",
    "demo_viome_test_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22334f13-2689-4be2-b4ee-873bbfc4a090",
   "metadata": {},
   "source": [
    "### CGM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89e47a-61de-4bc0-91ba-1519c953f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_test = pd.read_csv('cgm_test.csv')\n",
    "cgm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bb4a3-1358-47f5-805d-9233c2246ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a8609-0d71-4624-b0a7-873dde0bd7d2",
   "metadata": {},
   "source": [
    "### Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf7c6e-2fe1-4cb3-80b9-0f19e693fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = pd.read_csv('img_test.csv')\n",
    "img_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbc8f2-901d-417e-b329-f4e6e1c3d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b4b00-17bf-4e28-a7c5-20df30bdb5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in 'Breakfast Fiber' with the median\n",
    "img_test['Breakfast Fiber'] = img_test['Breakfast Fiber'].fillna(img_test['Breakfast Fiber'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01814ad-c1f3-4825-9046-737cfdc188a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c881b-b3c2-413b-887b-c4d0bd9b8f5f",
   "metadata": {},
   "source": [
    "### Merge the three dataframes and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60878459-5608-458f-bf3d-f3583a5aba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the CGM_data and Image_data based on 'Subject ID' and 'Day' columns\n",
    "merged_df_test = pd.merge(cgm_test, img_test, on=['Subject ID', 'Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3fe43-947a-474b-b6c0-41eb58abde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_test_wdemo_wviome = merged_df_test.merge(\n",
    "    demo_viome_test_processed,\n",
    "    on='Subject ID',  # Merge key\n",
    "    how='left'        # Keeps all rows from merged_df_train_wlabels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831c650-3c29-45bb-9133-fe267f8340c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_empty_values = merged_df_test_wdemo_wviome[merged_df_test_wdemo_wviome['Image Before Lunch'] == '[]']\n",
    "print(rows_with_empty_values['Image Before Lunch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3465d-18bc-4627-89e2-88a83c378fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_empty_values = merged_df_test_wdemo_wviome[merged_df_test_wdemo_wviome['Image Before Breakfast'] == '[]']\n",
    "print(rows_with_empty_values['Image Before Breakfast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908ec14-4f1b-4ff8-ae17-c1b4c248668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_empty_values = merged_df_test_wdemo_wviome[merged_df_test_wdemo_wviome['CGM Data'] == '[]']\n",
    "print(rows_with_empty_values['CGM Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf75da-6b54-45de-9f29-b87e7e84e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing and interpolation\n",
    "merged_df_test_wdemo_wviome[\"CGM Data\"] = merged_df_test_wdemo_wviome[\"CGM Data\"].apply(parse_cgm_data)  # Parse CGM Data\n",
    "merged_df_test_wdemo_wviome[\"CGM Data Per Minute\"] = merged_df_test_wdemo_wviome[\"CGM Data\"].apply(interpolate_cgm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c67f5-c274-4987-8cf2-79feedda297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset_test(Dataset):\n",
    "    def __init__(self, dataframe, image_columns, cgm_columns, viome_columns, demo_columns, transform=None, target_transform=None):\n",
    "        self.df = dataframe\n",
    "        self.image_columns = image_columns\n",
    "        self.cgm_columns = cgm_columns\n",
    "        self.viome_columns = viome_columns\n",
    "        self.demo_columns = demo_columns\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        # Process image data\n",
    "        image_data = self.df.iloc[idx][self.image_columns[0]]\n",
    "        image = ast.literal_eval(image_data)\n",
    "        image_array = np.array(image)\n",
    "        image_array = image_array / 255 # Normalize the pixel values between 0 and 1\n",
    "        image_tensor = torch.tensor(image_array, dtype=torch.float32).permute(2, 0, 1)  # Shape: (C, H, W)\n",
    "        # Process CGM time-series data (ignore timestamps)\n",
    "        cgm_data = self.df.iloc[idx][self.cgm_columns]\n",
    "        #print(cgm_data)\n",
    "        cgm_values = [entry[1] for entry in cgm_data]\n",
    "        cgm_array = np.array(cgm_values).reshape(-1, 1)\n",
    "        cgm_tensor = torch.tensor(cgm_values, dtype=torch.float32)  # Shape: (time_series_length,)\n",
    "        # adding Viome data\n",
    "        viome_data = self.df.iloc[idx][self.viome_columns[0]]\n",
    "        #print(viome_data)\n",
    "        viome_tensor = torch.tensor(viome_data, dtype=torch.float32)\n",
    "        demo_data = self.df.iloc[idx][self.demo_columns]\n",
    "        #print(f\"Demo data at index {idx}: {demo_data}\") \n",
    "        demo_tensor = torch.tensor(demo_data, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "\n",
    "        return image_tensor, cgm_tensor, viome_tensor, demo_tensor\n",
    "\n",
    "def collate_test_fn(batch):\n",
    "    images, cgm_tensors, viomes, demos = zip(*batch)\n",
    "\n",
    "    # Stack image tensors and labels\n",
    "    images = torch.stack(images)  # Shape: (batch_size, C, H, W)\n",
    "\n",
    "    viomes = torch.stack(viomes)\n",
    "    demos = torch.stack(demos)\n",
    "\n",
    "    # Pad CGM tensors to the max_length, # Shape: (batch_size, max_seq_len)\n",
    "    max_length = 750\n",
    "    cgm_tensors = pad_sequence([F.pad(tensor, (0, max_length - len(tensor)), \"constant\", 0)\n",
    "                               for tensor in cgm_tensors], batch_first=True)\n",
    "\n",
    "    # Reshape CGM tensor to add an 'input_size' dimension (1 for single feature per time-step)\n",
    "    cgm_tensors = cgm_tensors.unsqueeze(-1)  # Shape: (batch_size, max_seq_len, 1)\n",
    "    viomes = viomes.unsqueeze(-1)\n",
    "\n",
    "    demos = demos.unsqueeze(-1)\n",
    "    return images, cgm_tensors, viomes, demos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b26340-4161-44f7-a36c-bc43d0411935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MultiModalDataset\n",
    "dataset_test = MultiModalDataset_test(\n",
    "    dataframe=merged_df_test_wdemo_wviome,  \n",
    "    image_columns=image_columns,\n",
    "    cgm_columns=cgm_columns,  \n",
    "    viome_columns=viome_columns,\n",
    "    demo_columns = demo_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b8793-aba7-4033-87f1-29a30e097ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset_test, batch_size=10, shuffle=True, collate_fn=collate_test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edf404-cbe8-40e1-bd6a-c734b0f959d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test phase\n",
    "\n",
    "# List to store predictions\n",
    "y_test_pred = []\n",
    "\n",
    "multimodal_model.eval()\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, cgm_tensors, viomes, demos) in enumerate(test_loader):\n",
    "        images, cgm_tensors, viomes, demos = images.to(device), cgm_tensors.to(device), viomes.to(device), demos.to(device)\n",
    "\n",
    "        img_embedding, cgm_embedding, viome_embedding, demo_embedding = multimodal_model([images, cgm_tensors, viomes, demos])\n",
    "        combined_embedding = torch.cat((img_embedding, cgm_embedding, viome_embedding, demo_embedding), dim=-1)\n",
    "        predictions = nutrient_predictor(combined_embedding)\n",
    "\n",
    "        # Append predictions to list\n",
    "        y_test_pred.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66032adc-196d-40a3-9267-3f5bf03c4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to a numpy array\n",
    "y_test_pred = np.array(y_test_pred)\n",
    "# Flatten the array to 1D\n",
    "y_test_pred_flattened = y_test_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e977cbd-e2ed-433e-ba31-5348962a8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = pd.read_csv('label_train.csv')\n",
    "# denormalize label by label mean\n",
    "cal_train_mean = label_train['Lunch Calories'].mean()\n",
    "cal_pred = y_test_pred_flattened*cal_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b2a19-b1ac-43f2-8765-df749e59cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"row_id\": range(0, len(cal_pred)),\n",
    "    \"label\": cal_pred      \n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"test_preds.csv\", index=False)  # index=False to exclude the DataFrame index\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef17a7-6e1f-4d18-8a05-c1b09bb1faae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
